import requests

url = "https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2023-05-29"

body = {
	"messages": [{"role":"system","content":"You are an intelligent AI programming assistant, utilizing a Granite code language model developed by IBM. Your primary function is to assist users in programming tasks, including code generation, code explanation, code fixing, generating unit tests, generating documentation, application modernization, vulnerability detection, function calling, code translation, and all sorts of other software engineering tasks. You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is correct given the context and user query, and that it is grounded in the context. Furthermore, make sure that the response is supported by the given document or context. Always make sure that your response is relevant to the question. If an explanation is needed, first provide the explanation or reasoning, and then give the final answer. Avoid repeating information unless asked."},{"role":"user","content":[{"type":"text","text":"Please write a python code of Embedding Data with SentenceTransformers for heavy rainfall which  is causing flash flooding"}]},{"role":"assistant","content":"This Python library encodes both climate event descriptions and user queries into dense vector embeddings, enabling semantic similarity search in Elasticsearch.\n\n2.3 Elasticsearch Python SDK\n\nElasticsearch stores event embeddings and supports vector-based semantic search, allowing responders to find relevant events rapidly.\n\n2.4 IBM Watsonx.ai\n\nWatsonx.ai is IBM’s generative AI platform that produces natural language answers from user questions and context provided by the RAG pipeline.\n\n2.5 LangChain RAG Pipeline\n\nLangChain orchestrates retrieval and generation by integrating the Elasticsearch retriever and Watsonx.ai. It constructs prompts combining user questions with relevant climate event data, then generates concise, actionable answers.\n\n\n\n3. Technical Implementation\n\n3.1 Data Ingestion from IBM EIS\n\nimport requests\n\nimport os\n\nfrom dotenv import load_dotenv\n\n\n\nload_dotenv()\n\nEIS_API_KEY = os.getenv(\"EIS_API_KEY\")\n\n\n\ndef fetch_eis_data(event_type=\"heavy_rain\"):\n\n    url = f\"https://api.eis.ibm.com/v1/events?eventType={event_type}\"\n\n    headers = {\"Authorization\": f\"Bearer {EIS_API_KEY}\"}\n\n    response = requests.get(url, headers=headers)\n\n    if response.status_code == 200:\n\n        with open(f\"data/eis_reports/{event_type}.json\", \"w\") as f:\n\n            f.write(response.text)\n\n    else:\n\n        print(f\"Error: {response.status_code} - {response.text}\")\n\n3.2 Embedding Data with SentenceTransformers\n\npython\n\nCopyEdit\n\nfrom sentence_transformers import SentenceTransformer\n\n\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ndescription = \"Heavy rainfall in Nairobi caused flash flooding on June 3, 2025.\"\n\nembedding = model.encode(description)\n\n3.3 Indexing with Elasticsearch Python SDK\n\npython\n\nCopyEdit\n\nfrom elasticsearch import Elasticsearch\n\n\n\nes = Elasticsearch(\n\n    hosts=[os.getenv(\"ELASTICSEARCH_HOST\")],\n\n    basic_auth=(os.getenv(\"ELASTICSEARCH_USERNAME\"), os.getenv(\"ELASTICSEARCH_PASSWORD\"))\n\n)\nes = Elasticsearch(\n\n    hosts=[os.getenv(\"ELASTICSEARCH_HOST\")],\n\n    basic_auth=(os.getenv(\"ELASTICSEARCH_USERNAME\"), os.getenv(\"ELASTICSEARCH_PASSWORD\"))\n\n)\n\ndoc = {\"description\": description, \"embedding\": embedding.tolist()}\n\nes.index(index=\"climate_knn_index\", body=doc)\n\n3.4 Integrating LangChain and Watsonx.ai\n\npython\n\nCopyEdit\n\nfrom ibm_watsonx_ai import Credentials, WatsonxAI\n\nfrom langchain.embeddings import SentenceTransformerEmbeddings\n\nfrom langchain.vectorstores import ElasticVectorSearch\n\nfrom langchain.chains import RetrievalQA\n\n\n\ncredentials = Credentials(\n\n    url=\"https://us-south.ml.cloud.ibm.com\",\n\n    api_key=os.getenv(\"WATSONX_API_KEY\")\n\n)\n\nllm = WatsonxAI(credentials, project_id=os.getenv(\"WATSONX_PROJECT_ID\"))\n\nembedding_fn = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n\n\nvectorstore = ElasticVectorSearch(\n\n    elasticsearch_url=os.getenv(\"ELASTICSEARCH_HOST\"),\n\n    index_name=\"climate_knn_index\",\n\n    embedding=embedding_fn,\n\n    es_user=os.getenv(\"ELASTICSEARCH_USERNAME\"),\n\n    es_password=os.getenv(\"ELASTICSEARCH_PASSWORD\")\n\n)\n\n\n\nqa_chain = RetrievalQA.from_chain_type(\n\n    llm=llm,\n\n    chain_type=\"stuff\",\n\n    retriever=vectorstore.as_retriever()\n\n)\n\nquestion = \"What measures should be taken to assist flood-affected communities in Nairobi?\"\n\nanswer = qa_chain.run(question)\n\nprint(answer)\n\n\n\n4. Case Studies\n\nCase Study 1: Flood Response in Nairobi\n\nScenario: Heavy rains on June 3, 2025, cause severe flooding in Nairobi’s Mathare settlement.Problem: Response teams must quickly decide whether to evacuate residents.Solution:\n\nIBM EIS provides real-time flood event data.\n\nSentenceTransformers encodes event descriptions.\n\nElasticsearch retrieves similar past flooding events.\n\nWatsonx.ai generates actionable guidance: “Based on past events, evacuation is recommended due to high flood risk in Mathare. Provide temporary shelters in adjacent districts.”\n\n\n\nCase Study 2: Earthquake Preparedness in Uganda\n\nClimate-RAG-ESG: Leveraging AI to Enhance Climate Disaster Response\n\nExecutive Summary\n\nAs climate change intensifies, extreme weather events such as floods, heavy rains, and earthquakes pose increasing risks to vulnerable communities worldwide. Traditional approaches to disaster response often lack timely, data-driven insights, leaving decision-makers ill-equipped to respond effectively. This whitepaper presents Climate-RAG-ESG, a Retrieval-Augmented Generation (RAG) pipeline that integrates IBM Environmental Intelligence Suite (EIS), IBM Watsonx.ai, Elasticsearch Python SDK, SentenceTransformers, and LangChain to empower disaster response teams with real-time, actionable answers to climate disaster questions.\n\n\n\n1. Introduction\n\nClimate disasters have become more frequent and severe, threatening human lives, disrupting economies, and exacerbating existing vulnerabilities. Responding to such disasters requires fast, reliable, and context-specific information. Existing solutions, however, often rely on static reports, manual data analysis, or generic AI chatbots that lack real-time event data, limiting their usefulness in urgent scenarios.\n\nClimate-RAG-ESG addresses these challenges by fusing real-time climate data from IBM EIS with cutting-edge AI tools, enabling disaster response teams to:✅ Access context-rich, natural language answers tailored to their region and event type.✅ Retrieve up-to-date climate event data quickly and efficiently.✅ Integrate AI insights directly into decision-making processes to save lives and enhance resilience.\n\n\n\n2. System Overview\n\n2.1 IBM Environmental Intelligence Suite (EIS)\n\nEIS provides real-time and historical climate event data, including event descriptions, locations, severity levels, and timestamps. Data is accessed via RESTful APIs and stored in JSON format for downstream processing.\n\n2.2 SentenceTransformers\n\nThis Python library encodes both climate event descriptions and user queries into dense vector embeddings, enabling semantic similarity search in Elasticsearch.\n\n2.3 Elasticsearch Python SDK\n\nElasticsearch stores event embeddings and supports vector-based semantic search, allowing responders to find relevant events rapidly.\n\n2.4 IBM Watsonx.ai\n\nWatsonx.ai is IBM’s generative AI platform that produces natural language answers from user questions and context provided by the RAG pipeline.\n\n2.5 LangChain RAG Pipeline\n\nLangChain orchestrates retrieval and generation by integrating the Elasticsearch retriever and Watsonx.ai. It constructs prompts combining user questions with relevant climate event data, then generates concise, actionable answers.\n\n\n\n3. Technical Implementation\n\n3.1 Data Ingestion from IBM EIS\n\nimport requests\n\nimport os\n\nfrom dotenv import load_dotenv\n\n\n\nload_dotenv()\n\nEIS_API_KEY = os.getenv(\"EIS_API_KEY\")\n\n\n\ndef fetch_eis_data(event_type=\"heavy_rain\"):\n\n    url = f\"https://api.eis.ibm.com/v1/events?eventType={event_type}\"\n\n    headers = {\"Authorization\": f\"Bearer {EIS_API_KEY}\"}\n\n    response = requests.get(url, headers=headers)\n\n    if response.status_code == 200:\n\n        with open(f\"data/eis_reports/{event_type}.json\", \"w\") as f:\n\n            f.write(response.text)\n\n    else:\n\n        print(f\"Error: {response.status_code} - {response.text}\")\n\n3.2 Embedding Data with SentenceTransformers\n\npython\n\nCopyEdit\n\nfrom sentence_transformers import SentenceTransformer\n\n\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ndescription = \"Heavy rainfall in Nairobi caused flash flooding on June 3, 2025.\"\n\nembedding = model.encode(description)\n\n3.3 Indexing with Elasticsearch Python SDK\n\npython\n\nCopyEdit\n"}],
	"project_id": "61b3fbe5-4a38-41ca-a07f-ffd1da5223d0",
	"model_id": "ibm/granite-20b-code-instruct",
	"frequency_penalty": 0,
	"max_tokens": 2000,
	"presence_penalty": 0,
	"temperature": 0,
	"top_p": 1
}

headers = {
	"Accept": "application/json",
	"Content-Type": "application/json",
	"Authorization": "Bearer YOUR_ACCESS_TOKEN"
}

response = requests.post(
	url,
	headers=headers,
	json=body
)

if response.status_code != 200:
	raise Exception("Non-200 response: " + str(response.text))

data = response.json()
